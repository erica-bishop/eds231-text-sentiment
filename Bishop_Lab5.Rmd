---
title: "Lab 5"
author: "Erica Bishop"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
library(tidytext)
library(tidyverse)
library(widyr)
library(irlba) 
library(broom) 
library(textdata)
library(ggplot2)
library(dplyr)
library(LexisNexisTools)
```

## Train Your Own Embeddings

### 1. Using the data from your Nexis Uni query from Week 2, create a set of word embeddings. To do this, you'll essentially need to recreate the steps in today's example down through the chunk named "pmi".

```{r read data}
#read in data (from lab 3)
all_files <- list.files(pattern = ".docx"
                       , path = "/Users/ericabishop/Documents/MEDSspring/EDS231/lab3_docs"
                       , full.names = TRUE
                       , recursive = TRUE
                       , ignore.case = TRUE)

#reading in LexisNexis file
LNT_dat <- lnt_read(all_files)

# Separate data types into different dfs
meta_df <- LNT_dat@meta
articles_df <- LNT_dat@articles 
paragraphs_df <- LNT_dat@paragraphs

#create a df from the lNT objects
orcas_df <- tibble(id = articles_df$ID,
                   Date = meta_df$Date,
                   Headline = meta_df$Headline,
                   text = articles_df$Article)

#clean the df a bit
ocras_df <- orcas_df %>% 
  mutate(text = str_replace(text, "Classification Language:.*", "")) %>%
  mutate(text = str_replace(text, "Contact:.*", "")) %>%
  mutate(text = str_replace(text, "Email:.*", "")) %>%
  mutate(text = str_replace(text, "COMMENTS.*", ""))


```

```{r unnest tokens}

#create unigrams from words in 
unigram_probs <- orcas_df %>% 
  unnest_tokens(word, text) %>%  #isolate words as tokens
  anti_join(stop_words, by = 'word') %>%  #remove stop words
  count(word, sort = T) %>%  #count total word frequencies and sort
  mutate(p = n/sum(n)) #add a column for the probabilities of each word

skipgrams <- orcas_df %>% 
  #can use unnest tokens to set gram of any length (unigram or here 5-gram)
  unnest_tokens(ngram, text, token = "ngrams", n = 5) %>% 
  mutate(ngramID = row_number()) %>% 
  tidyr::unite(skipgramID, id, ngramID) %>% 
  #now unnest 5-grams into words
  unnest_tokens(word, ngram) %>% 
  anti_join(stop_words, by = "word")
#windows are different lengths because where there was a stop word there is now no entry
#here doing stop words after windows, but COULD remove before but that changes the meaning (removing after like this preserves more meaning)

```

```{r probabilities}

#calculate probabilities
skipgram_probs <- skipgrams %>% 
  pairwise_count(word, skipgramID, diag = T, sort = T) %>% 
  mutate(p = n/sum(n))

#calculate normalized probabilities
normalized_prob <- skipgram_probs %>% 
  filter(n>20) %>% #look just at words that occur more than 20 times together
  rename(word1 = item1,
         word2 = item2) %>% 
  left_join(unigram_probs %>% 
              select(word1 = word, p1 = p),
            by = "word1") %>% 
  left_join(unigram_probs %>% 
              select(word2 = word, p2 = p),
            by = "word2") %>% 
  mutate(p_together = p/p1/p2)

#I still don't quite get why its valuable to look at word probabilities with themselves
#so removing all the instances of words occuring with themselves
#remove wohere word1 = word2
normalized_prob <- normalized_prob %>% 
  filter(word1 != word2)


```

```{r pmi}
#calculate pmi (using log 10?)
pmi_matrix <- normalized_prob %>% 
  mutate(pmi = log10(p_together)) %>% 
  #go from tidy to sparse matrix
  cast_sparse(word1, word2, pmi)

#this matrix is just 713 x 713 (smaller than climbing one)
dim(pmi_matrix)

```

```{r word vectors}

pmi_matrix@x[is.na(pmi_matrix@x)] <- 0 #cant operate on NAs so safegaurd by replacing with 0s

#decomposition
pmi_svd <- irlba(pmi_matrix,
                 100,
                 maxit = 500) #truncate iterations so it doesn't run forever :)

word_vectors <- pmi_svd$u #extract u matrix (u, single value, v are three)

rownames(word_vectors) <- rownames(pmi_matrix) #set rownames so we know what those words are

```

### 2. Think of 3-5 key words in your data set. Calculate and plot the 10 most semantically similar words for each of them. Identify and interpret any interesting or surprising results.

The words most closely related to my search terms (puget sound orcas) pop up the most, but I want to look into some subtopics:

-    salmon

-   noise

-   captivity

```{r syn-function}
search_synonyms <- function(word_vectors, selected_vector) {
dat <- word_vectors %*% selected_vector
    
similarities <- dat %>%
        tibble(token = rownames(dat), similarity = dat[,1])
similarities %>%
       arrange(-similarity) %>%
        select(c(2,3))
}
```

```{r salmon words}
salmon <- search_synonyms(word_vectors,
                        word_vectors["salmon",])

salmon_10 <- salmon %>% 
  top_n(10, similarity) %>% 
  mutate(token = reorder(token, similarity))


ggplot(data = salmon_10,
  aes(y = token,
      x = similarity,
      )) +
  geom_col(show.legend = FALSE,
           position = "stack",
           fill = "#fa9073") +
  theme(strip.text=element_text(hjust=0, size=12)) +
  labs(x = NULL, title = "What word vectors are most similar to salmon?")+
  theme_minimal()

```


```{r noise words}

noise <- search_synonyms(word_vectors,
                        word_vectors["noise",])
noise_10 <- noise %>% 
  top_n(10, similarity) %>% 
  mutate(token = reorder(token, similarity))


ggplot(data = noise_10,
  aes(y = token,
      x = similarity,
      )) +
  geom_col(show.legend = FALSE,
           position = "stack",
           fill = "#50545c") +
  theme(strip.text=element_text(hjust=0, size=12)) +
  labs(x = NULL,
       title = "What word vectors are most similar to noise?")+
  theme_minimal()


```


```{r captivity words}

captivity <- search_synonyms(word_vectors,
                        word_vectors["captivity",])

captivity_10 <- captivity %>% 
  top_n(10, similarity) %>% 
  mutate(token = reorder(token, similarity))


ggplot(data = captivity_10,
  aes(y = token,
      x = similarity,
      )) +
  geom_col(show.legend = FALSE,
           position = "stack",
           fill = "#03857e") +
  theme(strip.text=element_text(hjust=0, size=12)) +
  labs(x = NULL, title = "What word vectors are most similar to noise?",
       y = "Similarity")+
  theme_minimal()



```

3.  Assemble 3 word math equations that you think could be useful or interesting for exploring the meaning of key words or ideas in your data set.

```{r word math}

#maybe ocean or wwater for Puget Sound?
puget_sound <- word_vectors["puget",] + word_vectors["sound",]
#search for synonyms
puget_sound_words <- search_synonyms(word_vectors, puget_sound)
gt::gt(head(puget_sound_words)) #print table

#maybe fish plus chinook is salmon?
fish_chinook <- word_vectors["fish",] + word_vectors["chinook",]
fish_words <- search_synonyms(word_vectors, fish_chinook)
gt::gt(head(fish_words))

#will navy + pollution = noise?
navy_pollution <- word_vectors["navy",] + word_vectors["pollution",]
navy_words <- search_synonyms(word_vectors, navy_pollution)
gt::gt(head(navy_words))

```

The word math definitely spits out similar words, but I think they are all so tightly linked in a small dataset on a very limited topc. The navy + pollution word math I think had the most specific and interesting results. The fish + chinook also output salmon as the top result which is cool - although some of the other words were less meaningful. 

#### Pretrained Embeddings

4.  Following the example in the SMLTR text (section 5.4), create a set of 100-dimensional GloVe word embeddings.
    These embeddings were trained by researchers at Stanford on 6 billion tokens from Wikipedia entries.

5.  Test them out with the cannonical word math equation on the GloVe embeddings: "berlin" - "germany" + "france" = ?

6.  Recreate parts 2 and 3 above using the the GloVe embeddings in place of the ones you trained.
    How do they compare?
    What are the implications for applications of these embeddings?
