---
title: "Lab 4"
author: "Erica Bishop"
date: "2023-05-09"
output: html_document
---

```{r packages, include = FALSE}
library(tidytext)
library(tidyverse)
library(tidymodels)
library(textrecipes)
library(naivebayes)
# library(discrim) # naive-bayes
library(kernlab) #kernlab engine
```

Lab 4 Assignment: Due May 9 at 11:59pm

```{r load_data, message=FALSE}

urlfile ="https://raw.githubusercontent.com/MaRo406/EDS-231-text-sentiment/main/data/climbing_reports_model_dat.csv"
incidents_df<-readr::read_csv(url(urlfile))

```

### 1. Select another classification algorithm

I'll use a support vector machine classification model becuase SVM's are efficient where there are a lot of featuers (i.e. a lot of tokens - as in this text analysis). 

### 2. Conduct an initial out-of-the-box model fit on the training data and prediction on the test test data.  Assess the performance of this initial model.

```{r split data}
#set seed for reproducibility
set.seed(567)

#transform data so category is two classes 
incidents2class <- incidents_df %>% 
  mutate(fatal = factor(if_else(is.na(Deadly),
                                "non-fatal",
                                "fatal")))

#split data to train model
incidents_split <- initial_split(incidents2class)

incidents_train <- training(incidents_split)
incidents_test <- testing(incidents_split)

```

```{r set model specifications}

#set recipe
recipe <- recipe(fatal ~ Text,
       data = incidents_train) %>% 
  step_tokenize(Text) %>% #turning each word in text into a different feature
  step_tokenfilter(Text, max_tokens = 1000) %>%  #max_tokens argument limits to just most frequent words (number of features you want)
  step_tfidf(Text) #term frequency individual document frequency takes into account tfdf metric


#use default engine and tuning parameters
svm_spec <- svm_rbf() %>% 
  set_engine("kernlab") %>% 
  set_mode("classification") # %>% 
# translate()
# might have to use 'translate'

#bundle into a workflow
svm_wf <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(svm_spec)

```

```{r fit, message=FALSE} 

svm_fit <- svm_wf %>% 
  fit(data = incidents_train)
  
```

```{r resampling evaluation}
#resampling because relatively small dataset
#use default 10-fold cross validation
incidents_fold <- vfold_cv(incidents_train)

#fit data to resamples
svm_rs_fit <- fit_resamples(
  svm_wf, 
  incidents_fold,
  control = control_resamples(save_pred = T)
)

```


```{r metrics}

svm_rs_metrics <- collect_metrics(svm_rs_fit)
svm_rs_predictions <- collect_predictions(svm_rs_fit)

#print out a cute table
gt::gt(svm_rs_metrics,
       caption = "SVM model performance with 10-fold cross validation")

```

```{r ROC plot}
svm_rs_predictions |> 
  group_by(id) |> 
  roc_curve(truth = fatal, .pred_fatal) |> 
  autoplot() +
  labs("Resamples", 
       title = "ROC curve for Climbing Incident Reports")


```

```{r confustion matrix}

conf_mat_resampled(svm_rs_fit, tidy = FALSE) |> 
  autoplot(type = "heatmap") +
scale_fill_gradient(low = "#ffffff", high = "#1cfc90")



```
In the confuston matrix above, we can see that the model accurately classified most of the non-fatal climbin accidents, however it misclassified 27 fatal accidents as fatal.

### 3. Select the relevant hyperparameters for your algorithm and tune your model.

For support vector machine models, the cost parameter can be tuned to determine how high the penalty for misclassification is. The default value, used above, is 1, but values can range from 0.01 to 1000.  Cost is the parameter (C) that softens or hardens the margin of the hyperplane and controls the bias variance tradeoff. 

The margin parameter for SVM only applies to regression models, so below I am only tuning cost.

```{r tuning, cache=TRUE}
#set up model spec again to tune cost
svm_tune_spec <- svm_rbf(
  cost = tune()
) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")

#set up search grid
set.seed(345) #set seed again for grid generation
#took way to long to run with 100, going to 30
cost_grid <- grid_regular(cost(), levels = 30) #no transformation used for now

#make another wf
svm_tune_wf <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(svm_tune_spec)

#use tune_grid to find optimal cost parameter
#use the resamples created above

#tune in parallel to run faster
doParallel::registerDoParallel()
#taking out grid and using default latin hypercube maybe faster???
tune_rs <- tune_grid(
  svm_tune_wf,
  resamples = incidents_fold,
  # grid = cost_grid,
  control = control_resamples(save_pred = T)
)

```

```{r tuning_metrics}

collect_metrics(tune_rs)
autoplot(tune_rs) +
  labs(
    title = "SVM Performance Across Cost Penalities"
  )

```
It's odd that ROC and accuracy don't align in the cost value that optimizes the model. In this case, I'm going to go with accuracy (by why are they so different??? Is it because I abandoned my tuning grid in favor of the default?)

```{r select best cost}

#select best cost parameter based on accuracy metric
tune_rs %>% 
  show_best("accuracy")
#best cost for accuracy is about 3.2

#for comparison look at ROC
tune_rs %>% 
  show_best("roc_auc")
#best cost is about 0.655

#select the best one - by one std error picks simplest model within one std error of numerically best
#I think by_one_std_error will minimize difference in optimum cost between roc and accuracy
opt_cost <- tune_rs %>%  
  select_by_one_std_err(metric = "accuracy", -cost)


```


### 4. Conduct a model fit using your newly tuned model specification.  What are the terms most highly associated with non-fatal reports?  What about fatal reports?

SVM models don't allow for easy variable importance assessment or extracton - I couldn't get extract, VIP or other methods to work here. 

```{r final fit}
#finalize workflow
final_svm <- finalize_workflow(svm_tune_wf, opt_cost)

#fit data to tuned model
fitted_svm <- fit(final_svm, incidents_train)

```


### 5. Predict fatality of the reports in the test set.  Compare this prediction performance to that of the Naive Bayes and Lasso models.

```{r fit test}

last_fit(final_svm, incidents_split) |> 
  collect_metrics()

```
The accuracy for my svm model was about 87% and the ROC metric was about 0.89 - This is similar but a slightly lower accuracy and ROC than the lasso model, but a slight improvement over the naieve bayes model from class. 
