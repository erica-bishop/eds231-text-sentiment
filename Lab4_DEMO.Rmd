---
title: "Lab 4"
author: "Mateo Robbins"
date: "2023-04-25"
output: html_document
---

```{r packages, include = FALSE}
library(tidytext)
library(tidyverse)
library(tidymodels)
library(textrecipes)
library(discrim) # naive-bayes
library(naivebayes)
```

This data set includes more possible predictors than the text alone, but for this model we will only use the text variable
```{r data}

urlfile ="https://raw.githubusercontent.com/MaRo406/EDS-231-text-sentiment/main/data/climbing_reports_model_dat.csv"
incidents_df<-readr::read_csv(url(urlfile))
```

Now we'll split our data into training and test portions

```{r split-data}
set.seed(1234)

#transform data so category is two classes 
incidents2class <- incidents_df %>% 
  mutate(fatal = factor(if_else(is.na(Deadly),
                                "non-fatal",
                                "fatal")))

incidents_split <- initial_split(incidents2class)

incidents_train <- training(incidents_split)
incidents_test <- testing(incidents_split)

```

We use recipe() to specify the predictor and outcome variables and the data.

```{r recipe}

incidents_rec <- recipe(fatal ~ Text,
       data = incidents_train)


```

Next we add some familiar pre-processing steps on our Text variable: tokenize to word level, filter to the most common words, and calculate tf-idf.

```{r pre-process}

recipe <- incidents_rec %>% 
  step_tokenize(Text) %>% #turning each word in text into a different feature
  step_tokenfilter(Text, max_tokens = 1000) %>%  #max_tokens argument limits to just most frequent words (number of features you want)
  step_tfidf(Text) #term frequency individual document frequency takes into account tfdf metric

```

Create  tidymodels workflow to combine the modeling components

```{r workflow}

incidents_wf <- workflow() %>% 
  add_recipe(recipe)

```

```{r nb-spec}

#nb for naive bayesian
nb_spec <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("naivebayes")

```

Now we are ready to add our model to the workflow and fit it to the training data

```{r fit-model}

nb_fit <- incidents_wf %>% 
  add_model(nb_spec) %>% 
  fit(data = incidents_train)


```
Next up is model evaluation. We'll stretch our training data further and use resampling to evaluate our naive Bayes model. Here we create 10-fold cross-validation sets, and use them to estimate performance.

```{r cv_folds}

incidents_folds <- vfold_cv(incidents_train)

```

```{r nb-workflow}

nb_wf <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(nb_spec)

nb_wf

#now we have preprocessing, model, engine all wrapped up

```

To estimate its performance, we fit the model to each of the resampled folds, and then evaluate on the heldout part of each resampled fold.

```{r fit-resamples}

nb_rs <- fit_resamples(
  nb_wf,
  incidents_folds,
  control = control_resamples(save_pred = TRUE)
)

```

Extract the relevant information using collect_metrics() and collect_predictions() and examine the performance metrics.

```{r performance}

nb_rs_metrics <- collect_metrics(nb_rs)
nb_rs_predictions <- collect_predictions(nb_rs)

nb_rs_metrics

nb_rs_predictions

```

We'll use two performance metrics: accuracy and ROC AUC.
Accuracy is the proportion of the data that is predicted correctly. 
The ROC curve plots the true positive rate against the false positive rate; AUC closer to 1 indicates a better-performing model, while AUC closer to 0.5 indicates a model that does no better than random guessing.

```{r performance-plot}
#reciever operator curve

nb_rs_predictions %>% 
  group_by(id) %>% 
  roc_curve(truth = fatal,
            .pred_fatal) %>% 
  autoplot() %>% 
  labs("Resamples",
       title = "ROC for Climbing Incident Reports"
  )

```

Above, the ROC for each of the ten folds is plotted. The closer to the truth, the higher the curve. The diagonal represents the baseline probability from the training data. 

Another model method involves the confusion matrix. A confusion matrix tabulates a model's false positives and false negatives for each class.

```{r confusion-matrix}

conf_mat_resampled(nb_rs,
                   tidy = FALSE) %>% 
  autoplot(type = "heatmap") 

```
confusion matrix shows average across all documents
```{r null-model}

```